{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import spacy\n",
    "import argparse\n",
    "from utils_transform import *\n",
    "from transformer import *\n",
    "import pdb\n",
    "import time, datetime\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from eval_lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ArgTuple = namedtuple('args', 'size')\n",
    "args = ArgTuple(size=\"mid\")\n",
    "args.size\n",
    "params = Params(args)\n",
    "params.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: train_200k.csv, Val: val_10k.csv, test: test_10k.csv\n",
      "Vocab size: 10000\n",
      "Building vocab...\n",
      "TR vocab size: 9718, EN vocab size: 10004\n",
      "Done building vocab\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: %s, Val: %s, test: %s\" % (params.train_csv, params.val_csv, params.test_csv))\n",
    "print(\"Vocab size: %d\" % (params.vocab_size))\n",
    "train, val, test, TR, EN = load_train_val_test_datasets(params)\n",
    "pad_idx = EN.vocab.stoi[PAD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_tokenize_en = lambda tokenized: [EN.vocab.itos[i] for i in tokenized]\n",
    "rev_tokenize_tr = lambda tokenized: [TR.vocab.itos[i] for i in tokenized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_src = torch.tensor([[  5,   8,  19,  56, 539,   3,  46, 328,  39,   8,   4],\n",
    "        [  6,  43,   7, 506, 353,  45, 270,  15,   9,  24,   2],\n",
    "        [195, 137,  67, 249,  34,   3,  70,  55, 152,  16,   2],\n",
    "        [ 94,  21,  33,  14,  86, 132, 293,  66, 190, 227,   2],\n",
    "        [241, 308,  74, 359,  80,  48,  27,  64,  17,  14,   4],\n",
    "        [115,   7, 142,  40, 239,  74, 271,  58, 117,  22,   2],\n",
    "        [ 93,  62, 329,  33,  89,   5,   2,  96,  39, 164,   2],\n",
    "        [242,  42,  54, 124,   3,  63,  10, 122,  30, 291,   2],\n",
    "        [ 60,  11, 236, 124, 130, 158, 338, 416,  20,   8,   2],\n",
    "        [685, 370,  94, 305, 116,  20,   8,  17, 160,  19,   4],\n",
    "        [ 18, 287,  92,   9, 220, 179, 123, 297, 341,  86,   2],\n",
    "        [ 25, 160,  97,  26, 149,   7, 227,  16,  81,  34,  13],\n",
    "        [193,  12, 532,  31, 116,  21, 126,  16,  98,  11,   2],\n",
    "        [145,  87,  46,  98, 224,   3, 101,  48,  17,  14,   4],\n",
    "        [221,  62,  23,  27,  11,   3, 230,   7,  52, 365,   4],\n",
    "        [553, 295, 141, 222, 274,  73, 105, 127, 185, 120,   2],\n",
    "        [ 47, 262,  56,  87, 136, 618,  58,  35,  15,  34,   4],\n",
    "        [  6,  57,  35,   6,  71,   3,  46, 298, 282,  32,   2],\n",
    "        [ 60,  74,  66,  71,  70, 287, 144,  16,  98,  11,   2],\n",
    "        [  5,  24,  40,  51,  74, 108, 163,  15,  19, 149,   2]])\n",
    "batch_context = torch.tensor([[159,   7,  63, 170,  93,  70,  21, 108, 178,  84, 170, 441,  38, 221,\n",
    "         105,  59,  37,  24,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [189,  36,  12, 399,  20,  14, 106,  81,  80,  48, 167,  31,  15,  45,\n",
    "          18,  43,  42,  84, 119, 106,  81,  81,  22,   4,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [ 65, 571,  18, 112,  14,   2,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [  5,  71, 158, 132, 544,   2,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [ 25,  21, 458,  75,  50,   2,   2,   2,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [ 63,  10, 122,  30, 291,   2,   2,   2,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [ 52,   5,   2,  29, 305,  69,  83,   8,  15,  19,   6,  95,  73,   4,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [  6,  19, 257,  18, 233, 418,  60,  11,  18,  55,  38,   2,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [  6,  80,  38, 356,  11,  88, 131, 242, 374,  16, 168, 220,  50, 112,\n",
    "          22,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [  6, 330, 131,  52, 196, 443, 150,  80, 304,  59,  37, 305, 116,  45,\n",
    "          20,   8,  26,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [ 44,   5, 215,  59,  12, 662,  14,  63, 448, 125,  74, 359, 136, 219,\n",
    "           3, 162,  38,  17,  32,   4,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [114, 112, 447, 146, 152,  37,  79, 133,   6,  24,  14,  93,  18, 149,\n",
    "          97,  15, 149, 202,  20,   8,   2,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [113,  57,   3, 380,  33,  21,  59, 486,  19, 296, 116,  42,  84,  81,\n",
    "          90, 220, 201,  28,  27,  22,   2,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [ 56,  36, 208,  28,  24,   4,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [ 44,  88, 189, 263, 127, 298,  61,  58,  35,   2,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [ 60, 250,  87,  79, 153, 172,  37,  79, 133, 210,  16,  22,   2,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [311,  75,  91, 478, 429,   3, 158,  62, 534,  42, 111,  54, 130, 295,\n",
    "         141, 142, 438, 340,  12,  14, 593,   2,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
    "        [ 56,  49, 265,   3,   5,  24,   9,  73, 323,  42, 455, 142,  40, 239,\n",
    "         317,  23, 213, 140,  92,  49,  27,  11,   3,   5,  24,  14,  46, 431,\n",
    "         178,  36,  27,  11,   2,   1,   1,   1,   1,   1,   1],\n",
    "        [ 18, 287,  92,   6, 344,  12, 107, 136,  42,  34, 496, 151,  82,   5,\n",
    "          95,  50, 284,   3, 228,  55,   6,  24, 138,  91, 185,  23,  12, 283,\n",
    "          11,  24,  14,  88,   5, 186,  20,   8, 184,  29,   2],\n",
    "        [  6,  82,  56,  87, 136, 116,  21,  59,  37,  24,   4,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
    "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9718 10004\n"
     ]
    }
   ],
   "source": [
    "print(len(TR.vocab), len(EN.vocab))\n",
    "pad_idx = EN.vocab.stoi[PAD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, time, copy\n",
    "import pdb\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        src, tgt, src_mask, tgt_mask = batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        src_context, src_context_mask = batch.src_context, batch.src_context_mask\n",
    "        encoder_output = self.encode(batch)\n",
    "        return self.decode(encoder_output, src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, batch):\n",
    "        if self.encoder.use_context:\n",
    "            return self.encoder(\n",
    "                self.src_embed(batch.src), batch.src_mask, \n",
    "                self.src_embed(batch.src_context), batch.src_context_mask)\n",
    "        else:\n",
    "            return self.encoder(self.src_embed(batch.src), batch.src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        embedded = self.tgt_embed(tgt)\n",
    "        return self.decoder(embedded, memory, src_mask, tgt_mask)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(layer.size, eps=1e-6)\n",
    "        self.use_context = False\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class CombinationLayer(nn.Module):\n",
    "    '''Combines the outputs of the source encoder and the context encoder.\n",
    "    '''\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(CombinationLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.context_attn = copy.deepcopy(self_attn)\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "        self.size = size\n",
    "        self.w = nn.Linear(2 * self.size, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, src_mask, context, context_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "\n",
    "        # From the source encoder, there's a self-connection sublayer.\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, src_mask))\n",
    "\n",
    "        # Fomr the context encoder.\n",
    "        context = self.sublayer[1](x, lambda x: self.context_attn(x, context, context, context_mask))\n",
    "\n",
    "        # Append context and X together\n",
    "        x_and_context = torch.cat([x, context], dim=2) # [batch x len x 2*size]\n",
    "        g = self.sigmoid(self.w(x_and_context))\n",
    "\n",
    "        print(g)\n",
    "\n",
    "        # Gated sum\n",
    "        gated_sum = g * x + (1 - g) * context\n",
    "        return self.sublayer[2](gated_sum, self.feed_forward)\n",
    "\n",
    "\n",
    "class EncoderWithContext(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(EncoderWithContext, self).__init__()\n",
    "        # N \"regular\" layers. Share embeddings\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(layer.size, eps=1e-6)\n",
    "        self.combination_layer = CombinationLayer(\n",
    "            layer.size, copy.deepcopy(layer.self_attn), \n",
    "            copy.deepcopy(layer.feed_forward), layer.dropout)\n",
    "        self.use_context= True\n",
    "        \n",
    "    def forward(self, x, mask, src_context, context_mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "\n",
    "        # The src only goes through N - 1 layers.\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src_context = layer(src_context, context_mask) \n",
    "\n",
    "        out = self.combination_layer(x, mask, src_context, context_mask)\n",
    "        return self.norm(out)\n",
    "\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn \n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.long().data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 1: # Empty tensors are dim 1 in Pytorch 0.4+\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        assert(not(true_dist.requires_grad))\n",
    "        return self.criterion(x, true_dist)\n",
    "\n",
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    model.src_embed[0].lut.weight = model.tgt_embed[0].lut.weight\n",
    "    # model.generator.lut.weight = model.tgt_embed[0].lut.weight\n",
    "    return model\n",
    "\n",
    "def make_context_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        EncoderWithContext(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    model.src_embed[0].lut.weight = model.tgt_embed[0].lut.weight\n",
    "    # model.generator.lut.weight = model.tgt_embed[0].lut.weight\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, batch, pad_idx):\n",
    "    \"\"\"Calculates the log likelihood of a batch, given the model.\n",
    "    \"\"\"\n",
    "    memory = model.encode(batch)\n",
    "    total_prob = torch.zeros(batch.trg_y.size(0))\n",
    "    for i in range(0, batch.trg_y.size(1)): # trg_len\n",
    "        y_prev = batch.trg[:, :i + 1]\n",
    "        out = model.decode(memory, batch.src_mask, \n",
    "                           y_prev.clone().detach(),\n",
    "                           (subsequent_mask(y_prev.size(1))\n",
    "                                    .type_as(batch.src.data)))\n",
    "        probs = model.generator(out[:, -1]) # batch x vocab\n",
    "        trg_index = batch.trg_y[:, i]\n",
    "        prob_of_trg = probs.gather(1, trg_index.view(-1,1)) # not sure about this\n",
    "        pad_mask = (trg_index != pad_idx).to(dtype=torch.float32)\n",
    "        total_prob += (prob_of_trg.squeeze()) * pad_mask\n",
    "        if i == 0:\n",
    "            init_probs = total_prob\n",
    "    return total_prob, init_probs\n",
    "\n",
    "def greedy_decode(model, batch, max_len, start_symbol):\n",
    "    src = batch.src, src_mask = batch.src_mask # This is just wrong lol\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    total_prob = 0.0\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           ys.clone().detach(),\n",
    "                           (subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        max_prob, next_word = torch.max(prob, dim = 1)\n",
    "        total_prob += max_prob.data\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "def beam_decode(model, src, src_mask, src_context, pad_idx, max_len, start_symbol, end_symbol, k=5):\n",
    "    \"\"\"Beam decoder.\n",
    "    \"\"\"\n",
    "    batch = Batch(src.unsqueeze(0), src_mask.unsqueeze(0), src_context.unsqueeze(0), pad_idx)\n",
    "    memory = model.encode(batch)\n",
    "    ys = torch.ones(1,1).fill_(start_symbol).type_as(src.data)\n",
    "    hypotheses = [(ys, 0.0)]\n",
    "    for i in range(max_len):\n",
    "      candidates_at_length = []\n",
    "      for hypothesis, previous_prob in hypotheses:\n",
    "        if hypothesis[0, -1] == end_symbol:\n",
    "          candidates_at_length.append((hypothesis, previous_prob))\n",
    "        else:\n",
    "          # feed through model\n",
    "          out = model.decode(memory, src_mask, \n",
    "                               hypothesis.clone().detach(),\n",
    "                               (subsequent_mask(hypothesis.size(1))\n",
    "                                        .type_as(src.data)))\n",
    "          probs = model.generator(out[:, -1])\n",
    "          # Keep track of top k predictions for each candidates\n",
    "          top_probs, predictions_at_step = torch.topk(probs, k, dim=1)\n",
    "          new_hypotheses = [torch.cat([hypothesis.clone(), pred.reshape(1,1)], dim=1) for pred in predictions_at_step.flatten()]\n",
    "          new_probs = top_probs.flatten().data + previous_prob\n",
    "          candidates_at_length = candidates_at_length + list(zip(new_hypotheses, new_probs))\n",
    "      hypotheses = sorted(candidates_at_length, key = lambda x: x[1], reverse=True)[:k]\n",
    "    return hypotheses[0][0].squeeze() # change to 1D tensor\n",
    "\n",
    "def eval_bleu(pad_idx, eval_iter, model, max_len, start_symbol, end_symbol, rev_tokenize_trg, bpemb_en):\n",
    "  \"\"\"Calculates average BLEU score of a model on some validation iterator.\n",
    "  \"\"\"\n",
    "  bleus = []\n",
    "  for old_batch in eval_iter:\n",
    "    batch = rebatch(pad_idx, old_batch)\n",
    "    for i in range(batch.src.size(0)): # batch_size\n",
    "      hypothesis = beam_decode(model, batch.src[i], batch.src_mask[i], batch.src_context[i],\n",
    "       pad_idx, max_len, start_symbol, end_symbol, k=5)[1:-1] # cut off SOS, EOS\n",
    "      targets = batch.trg_y[i, :-1] # Doesn't have SOS. Cut off EOS\n",
    "      hyp_str = bpemb_en.decode(rev_tokenize_trg(hypothesis))\n",
    "      trg_str = bpemb_en.decode(rev_tokenize_trg(targets))\n",
    "      bleu = get_moses_multi_bleu([hyp_str], [trg_str])\n",
    "\n",
    "      if bleu == None: # Error\n",
    "        print(i, hyp_str, trg_str)\n",
    "      else:\n",
    "        print(bleu, hyp_str, trg_str)\n",
    "        bleus.append(bleu)\n",
    "  return sum(bleus) / len(bleus)\n",
    "\n",
    "def eval_accuracy_helper(pad_idx, eval_iter, model):\n",
    "  \"\"\" Helper function that calculates how well the model chooses the correct\n",
    "  pronoun on the data in an iterator.\n",
    "  \"\"\"\n",
    "  n_correct = 0.0\n",
    "  n_total = 0.0\n",
    "  log_diff = 0.0\n",
    "  for b in eval_iter:\n",
    "    batch_correct, batch_incorrect = rebatch_for_eval(pad_idx, b)\n",
    "    res1 = log_likelihood(model, batch_correct, pad_idx)\n",
    "    res2 = log_likelihood(model, batch_incorrect, pad_idx)\n",
    "    probs = torch.stack([\n",
    "      res1[0], \n",
    "      res2[0]], dim=1) # n x 2\n",
    "    log_diff += sum(probs[:,0] - probs[:,1]).item()\n",
    "    correct = probs[:, 0] > probs[:, 1] # should assign higher probability to the left\n",
    "    print(\"sum of differences in log probs: \", probs[:, 0] - probs[:, 1])\n",
    "    n_correct += torch.sum(correct).item()\n",
    "    n_total += correct.size(0)\n",
    "  print(\"Correct: %d / %d = %f\" % (n_correct, n_total, (n_correct / n_total)))\n",
    "  print(\"Log difference %f\" % log_diff)\n",
    "  print(\"Log difference for first terms %f\" %  sum(res1[1] - res2[1]).item())\n",
    "  return\n",
    "\n",
    "def eval_accuracy(pad_idx, path_to_test_set, model, TR, EN):\n",
    "  \"\"\"Calculates how well the model chooses the correct pronoun, given\n",
    "  a dataset in TSV form. \n",
    "  \"\"\"\n",
    "  full_path = \"data/%s\" % (path_to_test_set)\n",
    "  # Must be in order\n",
    "  # !!! IMPT note there are five fields\n",
    "  data_fields = [\n",
    "  ('src_context', TR), ('src', TR),\n",
    "  ('trg_context', EN), ('trg_correct', EN), ('trg_incorrect', EN)]\n",
    "  print('Evaluating discriminative dataset: %s ' % (full_path))\n",
    "  test = TabularDataset(\n",
    "    full_path,\n",
    "    format='tsv', \n",
    "    fields=data_fields)\n",
    "\n",
    "  test_iter = Iterator(\n",
    "    test, batch_size=100, sort_key=lambda x: 1, repeat=False, train=False)\n",
    "  eval_accuracy_helper(pad_idx, test_iter, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path, tr_voc, en_voc, use_context):\n",
    "    \"\"\"Loads a trained model from memory for evaluation.\n",
    "    \"\"\"\n",
    "    if use_context:\n",
    "      model = make_context_model(tr_voc, en_voc, N=6)\n",
    "    else:\n",
    "      model = make_model(tr_voc, en_voc, N=6)\n",
    "    model.load_state_dict(torch.load(path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_model = load('models/0227_context_200k_debugged_40.pt', len(TR.vocab), len(EN.vocab), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating discriminative dataset: data/female_subject.tsv \n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of differences in log probs:  tensor([-0.5260, -0.5018, -0.8249, -0.9568, -0.7576, -0.4453, -0.8437, -0.8402,\n",
      "        -0.8317, -0.6583, -0.5270, -0.6199, -1.1213, -0.9185, -0.1819, -1.1137,\n",
      "        -0.6629, -0.5200, -0.6935, -0.8984, -0.8051, -0.9063, -0.8183, -0.6056,\n",
      "        -0.5316, -1.0984, -1.0953, -0.9057, -1.0835, -0.8377, -0.7518, -1.0776,\n",
      "        -1.7340, -0.5819, -0.6551, -0.6246, -0.3880, -0.6439, -0.7788, -0.8302],\n",
      "       grad_fn=<SubBackward0>)\n",
      "Correct: 0 / 40 = 0.000000\n",
      "Log difference -31.196846\n",
      "Log difference for first terms -31.196846\n"
     ]
    }
   ],
   "source": [
    "eval_accuracy(pad_idx, \"female_subject.tsv\", context_model, TR, EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = load('models/0220_baseline_200k_100.pt', len(TR.vocab), len(EN.vocab), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating discriminative dataset: data/female_subject.tsv \n",
      "sum of differences in log probs:  tensor([-0.8280, -1.6794, -0.8245, -1.8511, -1.0720, -0.7715, -0.5312, -1.0224,\n",
      "        -1.1075, -1.3326, -1.1280, -1.0859, -1.4362, -1.0091, -1.0416, -1.1151,\n",
      "        -0.7611, -0.1875, -1.2887, -1.1325, -0.8460, -1.2922, -2.0314, -0.9724,\n",
      "        -1.1152, -1.5892, -2.1911, -1.3357, -1.3766, -1.6188, -1.1225, -1.0069,\n",
      "        -2.4823, -1.0564, -1.1161, -1.0570, -1.0841, -0.8606, -1.1548, -1.4232],\n",
      "       grad_fn=<SubBackward0>)\n",
      "Correct: 0 / 40 = 0.000000\n",
      "Log difference -47.938416\n",
      "Log difference for first terms -47.938416\n"
     ]
    }
   ],
   "source": [
    "eval_accuracy(pad_idx, \"female_subject.tsv\", model_baseline, TR, EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_context = load('models/0301_context_boc_200k_10.pt', len(TR.vocab), len(EN.vocab), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating discriminative dataset: data/female_subject.tsv \n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of differences in log probs:  tensor([-1.1688, -1.3411, -0.4435, -1.4115, -0.4737,  0.4368, -1.7191,  0.9614,\n",
      "        -1.5919, -2.4081, -0.7998, -1.1531, -0.2525, -0.3300, -1.4130, -0.8722,\n",
      "        -1.3525, -1.6742, -2.3171, -0.7716, -1.1659, -3.0730, -0.3808, -1.0429,\n",
      "        -0.6962,  0.7966, -1.2891, -0.2843, -0.4878, -0.2663, -0.0737,  0.7109,\n",
      "        -0.4005, -0.1491,  0.0211, -2.0143, -0.0599, -1.3845, -0.7994, -0.7486],\n",
      "       grad_fn=<SubBackward0>)\n",
      "Correct: 5 / 40 = 0.125000\n",
      "Log difference -32.883102\n",
      "Log difference for first terms -32.883102\n"
     ]
    }
   ],
   "source": [
    "eval_accuracy(pad_idx, \"female_subject.tsv\", new_context, TR, EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9718"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TR.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 0.4.1",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
