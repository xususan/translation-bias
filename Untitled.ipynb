{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os.path\n",
    "import pdb\n",
    "import argparse\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "MAX_LEN = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "DE = data.Field(tokenize=utils.tokenize_de)\n",
    "EN = data.Field(tokenize=utils.tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish build vocab\n",
      "Done bucketing data\n"
     ]
    }
   ],
   "source": [
    "# Download dataset, build vocab\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "\n",
    "print(\"Finish build vocab\")\n",
    "\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))\n",
    "\n",
    "print(\"Done bucketing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "USE_CUDA = False\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.embedding_size = hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, self.embedding_size)\n",
    "        self.dropout_p = 0\n",
    "        self.n_layers = 1\n",
    "        self.rnn = nn.LSTM(\n",
    "            self.embedding_size, \n",
    "            hidden_size, \n",
    "            num_layers=self.n_layers, \n",
    "            dropout=self.dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(1)\n",
    "        embedded = self.embedding(input)\n",
    "        h_0 = self.init_hidden(batch_size)\n",
    "        output, hidden = self.rnn(embedded, h_0)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden =  torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return (hidden, hidden.clone())\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output) # Try other?\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.out(output)\n",
    "        # output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, hidden_size, input_vocab_size, output_vocab_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = EncoderRNN(input_vocab_size,hidden_size)\n",
    "        self.decoder = DecoderRNN(hidden_size, output_vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if USE_CUDA: source = source.cuda()\n",
    "\n",
    "        # Encode\n",
    "        output_encoder, hidden_encoder = self.encoder(source)\n",
    "\n",
    "        # Decode\n",
    "        output_decoder, hidden_decoder = self.decoder(target, hidden_encoder)\n",
    "\n",
    "        # Predict\n",
    "        return output_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(hidden_size=50, input_vocab_size = len(DE.vocab), output_vocab_size = len(EN.vocab))\n",
    "\n",
    "def validate(model, val_iter, criterion):\n",
    "    ''' Calculate perplexity on validation set.'''\n",
    "    model.eval()\n",
    "\n",
    "    AL = AverageLosses()\n",
    "\n",
    "    for i, batch in enumerate(val_iter):\n",
    "        scores = model(batch.src, batch.trg)\n",
    "        # Remove <s> from beginning of target\n",
    "        targets = batch.trg[1:]\n",
    "        # Remove </s> from end of source bc nothing to predict after that.\n",
    "        scores = scores[:-1]\n",
    "\n",
    "        # Reshape.\n",
    "        new_scr = scores.view(scores.size(0) * scores.size(1), -1)\n",
    "        new_trg = targets.view(new_scr.size(0))\n",
    "\n",
    "        loss = criterion(new_scr, new_trg)\n",
    "\n",
    "        # Count number of non-padding elements on target.\n",
    "        num_words = (new_trg != 1).sum()\n",
    "\n",
    "        AL.update(loss.data, n_obs=num_words)\n",
    "\n",
    "    return exp(loss.avg)\n",
    "def train(train_iter, val_iter, model, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        AL = AverageLosses()\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            loss = train_batch(model, batch, criterion, optimizer)\n",
    "            AL.update(loss)\n",
    "            \n",
    "            if i % 1000 == 10:\n",
    "                print('''Epoch [{e}/{num_e}]\\t Batch [{b}/{num_b}]\\t Loss: {l:.3f}'''.format(e=epoch+1, num_e=num_epochs, b=i, num_b=len(train_iter), l=AL.avg))\n",
    "\n",
    "        ppl = validate(model, val_iter, criterion)\n",
    "        print('''Epoch [{e}/{num_e}]\\t Perplexity: {ppl:.3f}'''.format(e=epoch+1, num_e=num_epochs, ppl=ppl))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]\t Batch [10/3722]\t Loss: 5.880\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.98), lr=0.5)\n",
    "train(train_iter, val_iter, model, nn.CrossEntropyLoss(), optimizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
